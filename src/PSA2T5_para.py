import json
import argparse
from transformers import T5Tokenizer
import itertools
from pprint import pprint
from tqdm.auto import tqdm
from multiprocessing import Pool
import os

case_roma_kana = {
	'ga':'が',
	'o':'を',
	'ni':'に'
}

def create_parser():
	parser = argparse.ArgumentParser()
	parser.add_argument('-psa','--psa_instance',help='path to psa_instance generated by NTC2PSAdataset.py')
	parser.add_argument('-o','--output_path',help='output path')
	parser.add_argument('-p','--process',help='number of process',default=4,type=int)

	return parser

def edit_argsurface(arg_dict:dict)->str:
	if(arg_dict['arg_type'] == 'none'):
		arg_surface = ''
		return arg_surface
	elif(arg_dict['arg_type'] == 'exog'):
		arg_surface = '<extra_id_99>'
	elif(arg_dict['arg_type'] == 'exo1'):
		arg_surface = '<extra_id_98>'
	elif(arg_dict['arg_type'] == 'exo2'):
		arg_surface = '<extra_id_97>'
	else:
		arg_surface = arg_dict['arg_surface']
	surface = arg_surface + case_roma_kana[arg_dict['case_type']]
	return surface
def edit_goldchain(gold_chain:str)->set:
	if(gold_chain == ['exog']):
		new_chain = ['<extra_id_99>']
	elif(gold_chain == ['exo1']):
		new_chain = ['<extra_id_98>']
	elif(gold_chain == ['exo2']):
		new_chain = ['<extra_id_97>']
	else:
		new_chain = gold_chain
	return new_chain

def main(triplet:list)->str:
	ga_str,o_str,ni_str = triplet
	ga_dict = json.loads(ga_str)
	o_dict = json.loads(o_str) 
	ni_dict = json.loads(ni_str)
	dicts = [ga_dict,o_dict,ni_dict]

	context = ga_dict['context']
	pred_sent_idx = ga_dict['pred_sent_index']
	pred_indices = ga_dict['pred_indices']
	context[pred_sent_idx].insert(pred_indices[0],"<extra_id_0>")
	context[pred_sent_idx].insert(pred_indices[-1] + 2,"<extra_id_1>")
	concated_context = ''.join(list(itertools.chain.from_iterable(context)))
	padded_input_ids = tokenizer(concated_context,max_length=512,padding="max_length").input_ids
	truncated_input_ids = padded_input_ids[len(padded_input_ids)-512:]

	arg_and_case = ''
	gold_arguments = {}
	arg_types = {}
	
	for arg_dict in dicts:
		case_type = arg_dict['case_type']
		arg_and_case += edit_argsurface(arg_dict)
		gold_arguments[case_type] = edit_goldchain(arg_dict['goldchain'])
		arg_types[case_type]  = arg_dict['arg_type']
	pred_surface = ga_dict['pred_surface']
	label = arg_and_case + pred_surface
	label_ids = tokenizer(arg_and_case + pred_surface,max_length=30,padding="max_length").input_ids
	alt_type = ga_dict['alt_type']		

	t5_dict = {
		'input_ids':truncated_input_ids,
		'labels':label_ids,
		'gold_arguments':gold_arguments,
		'predicate':pred_surface,
		'arg_types':arg_types,
		'alt_type':alt_type
	}
	return json.dumps(t5_dict)
		
parser = create_parser()
args = parser.parse_args()

with open(args.psa_instance,mode='r') as f:
	lines = f.readlines()
	pred_triplet = [] # ガ,ヲ,ニの3つの情報をまとめて述語単位のリストとする
	for i in range(0,len(lines),3):
		pred_triplet.append(lines[i:i+3])
tokenizer = T5Tokenizer.from_pretrained('megagonlabs/t5-base-japanese-web')
result = []
with Pool(processes=args.process) as pool:
	r = pool.map(main,pred_triplet)
jsonl = '\n'.join(r)

output_path  = args.output_path
if not os.path.exists(output_path):
	os.mkdir(output_path)

with open(os.path.join(output_path,os.path.basename(args.psa_instance)),mode='w') as out:
	out.write(jsonl)