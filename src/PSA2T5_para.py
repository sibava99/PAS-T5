import json
import argparse
from transformers import T5Tokenizer
import itertools
from pprint import pprint
from tqdm.auto import tqdm
from multiprocessing import Pool
import os

case_roma_kana = {
	'ga':'が',
	'o':'を',
	'ni':'に'
}

def create_parser():
	parser = argparse.ArgumentParser()
	parser.add_argument('-psa','--psa_dir',help='path to psa_instance_dir generated by NTC2PSAdataset.py')
	parser.add_argument('-o','--output_dir',help='output path')
	parser.add_argument('-p','--process',help='number of process',default=4,type=int)
	parser.add_argument('-l','--max_length',help='maximum input sequence length',default=512,type=int)

	return parser

def edit_argsurface(arg:dict)->str:
	if(arg['arg_type'] == 'none'):
		return ''
	elif(arg['arg_type'] == 'exog'):
		arg_surface = '<extra_id_99>'
	elif(arg['arg_type'] == 'exo1'):
		arg_surface = '<extra_id_98>'
	elif(arg['arg_type'] == 'exo2'):
		arg_surface = '<extra_id_97>'
	else:
		arg_surface = arg['arg_surface']
	surface = arg_surface + case_roma_kana[arg['case_type']]
	return surface
def edit_goldchain(gold_chain:str)->set:
	if(gold_chain == ["exog"]):
		new_chain = ['<extra_id_99>']
	elif(gold_chain == ["exo1"]):
		new_chain = ['<extra_id_98>']
	elif(gold_chain == ["exo2"]):
		new_chain = ['<extra_id_97>']
	else:
		new_chain = gold_chain
	return new_chain

def main(triplet:list)->str:
	ga_str,o_str,ni_str = triplet
	ga_instance = json.loads(ga_str)
	o_instance = json.loads(o_str) 
	ni_instance = json.loads(ni_str)
	instances = [ga_instance,o_instance,ni_instance]

	context = ga_instance['context']
	pred_sent_idx = ga_instance['pred_sent_index']
	pred_indices = ga_instance['pred_indices']
	context[pred_sent_idx].insert(pred_indices[0],"<extra_id_0>")
	context[pred_sent_idx].insert(pred_indices[-1] + 2,"<extra_id_1>")
	# concated_context = ''.join(list(itertools.chain.from_iterable(context)))
	# padded_input_ids = tokenizer(concated_context,max_length=512,padding="max_length").input_ids
	# truncated_input_ids = padded_input_ids[len(padded_input_ids)-512:]
	
	context_ids = tokenizer([''.join(sent) for sent in context],add_special_tokens=False).input_ids
	input_ids = [tokenizer.eos_token_id]
	for sentence_ids in context_ids[::-1]:
		if len(input_ids) + len(sentence_ids) < args.max_length:
			input_ids = sentence_ids + input_ids
		else:
			break
	input_ids += [tokenizer.pad_token_id] * (512 - len(input_ids))
	truncated_input_ids = input_ids
	
	arg_and_case = ''
	gold_arguments = {}
	arg_types = {}
	
	for arg in instances:
		case_type = arg['case_type']
		arg_and_case += edit_argsurface(arg)
		gold_arguments[case_type] = edit_goldchain(arg['goldchain'])
		arg_types[case_type]  = arg['arg_type']
	pred_surface = ga_instance['pred_surface']
	label = arg_and_case + pred_surface
	label_ids = tokenizer(arg_and_case + pred_surface,max_length=30,padding="max_length").input_ids
	alt_type = ga_instance['alt_type']		

	t5_dict = {
		'input_ids':truncated_input_ids,
		'labels':label_ids,
		'gold_arguments':gold_arguments,
		'predicate':pred_surface,
		'arg_types':arg_types,
		'alt_type':alt_type,
		'context':context,
		'pred_sent_index':pred_sent_idx,
		'pred_indices':pred_indices,
		'pred_bunsetsu_index':ga_instance['pred_bunsetsu_index'],
		'ntc_path':os.path.basename(ga_instance['ntc_path'])
	}
	return json.dumps(t5_dict)
		
parser = create_parser()
args = parser.parse_args()

for split in ['train','test','dev']:
	print(f'Processing {split}-dataset')
	with open(os.path.join(args.psa_dir,split+'.psa.jsonl'),mode='r') as f:
		lines = f.readlines()
		pred_triplet = [] # ガ,ヲ,ニの3つの情報をまとめて述語単位のリストとする
		for i in range(0,len(lines),3):
			pred_triplet.append(lines[i:i+3])
	tokenizer = T5Tokenizer.from_pretrained('megagonlabs/t5-base-japanese-web')
	result = []
	with Pool(processes=args.process) as pool:
		r = pool.map(main,pred_triplet)
	jsonl = '\n'.join(r)

	output_dir  = args.output_dir
	if not os.path.exists(output_dir):
		os.mkdir(output_dir)

	with open(os.path.join(output_dir,split+'.psa.jsonl'),mode='w') as out:
		out.write(jsonl)