import json
import argparse
import glob
import os
from pprint import pprint
from tqdm import tqdm
from transformers import AutoTokenizer
from NTC2PSAdataset import make_dep_trees,determin_argtype

arg_index = {'dep':0,'intra':1,'inter':2,'exo':3,'none':4}
arg_inv_index = {0:'dep',1:'intra',2:'inter',3:'exo',4:'none'}

# decode_dict = {
# 					'output':argument,
# 					'gold_arguments':psa_instance['gold_arguments'][case],
# 					'case_name':case,
# 					'arg_type':psa_instance['arg_types'][case],
# 					'predicate':predicate,
# 					'pred_sent_index':psa_instance['pred_sent_index'],
# 					'pred_indices':psa_instance['pred_indices'],
# 					'alt_type':alt_type,
# 					'output_sentence':t5_tokenizer.decode(output),
# 					'context':psa_instance['context'],
# 					'ntc_path':psa_instance['ntc_path']
# 					}
def create_parser():
	parser = argparse.ArgumentParser()
	parser.add_argument('--decode','-d',help='path to decoded jsonl generated by generate.py')
	parser.add_argument('--ntc_dir','-n',help='path to NTC testset')
	parser.add_argument('--subword','-s',action='store_true',help='Matching subword endings are correct')
	parser.add_argument('--no_distinct',action='store_true',help="Don't distinct subword token and normal token")
	parser.add_argument('--bidirection','-b',action='store_true',help='出力がgoldを含むかgoldに出力が含まれれば正解とする')

	return parser

def print_scores(name:str,result):
	tp = len(result['tp'])
	fp = len(result['fp'])
	fn = len(result['fn'])
	
	precision = tp/(tp + fp) if tp + fp > 0 else 0
	recall = tp/(tp + fn)
	
	f1 = (2*recall*precision/(recall + precision)) if recall + precision != 0 else 0

	print(f'DataFrame = {name}\nprecision : {precision} ({tp}/{tp + fp})\nrecall : {recall} ({tp}/{tp + fn})\nf1 : {f1}\n')

def is_correct(output:str,gold_arguments:list,subword=False,no_distinct=False,bidirection=False)->bool:
	"""
	* subword : 評価の際にトークンをサブワード化した末尾が一致していれば正解とする
	* no_distinct : 評価の際に[#糸]と[糸]のようなサブワードトークンと単一トークンを区別しない
	"""
	output_token = output.translate(str.maketrans({chr(0x0021 + i): chr(0xFF01 + i) for i in range(94)})) #半角文字を全角文字に変換　
	gold_arguments = [gold.translate(str.maketrans({chr(0x0021 + i): chr(0xFF01 + i) for i in range(94)})) for gold in gold_arguments]
	if(subword == True):
		output_token = bert_tokenizer.tokenize(output_token)[-1]
		gold_arguments = [bert_tokenizer.tokenize(argument)[-1] for argument in gold_arguments]
		if(no_distinct):
			output_token = output_token.replace('#','')
			gold_arguments = [argument.replace('#','') for argument in gold_arguments]

	result = False
	if(bidirection):
		for gold in gold_arguments:
			if ((output_token in gold) or (gold in output_token)):
				result = True
	else:
		if output_token in gold_arguments:
			result = True
	return result

def search_argtype(psa_instance)->str:
	output = psa_instance['output'].translate(str.maketrans({chr(0x0021 + i): chr(0xFF01 + i) for i in range(94)}))
	if(output.startswith('＜ｅｘｔｒａ＿ｉｄ＿９９＞')):
		return 'exog'
	elif(output.startswith('＜ｅｘｔｒａ＿ｉｄ＿９８＞')):
		return 'exo1'
	elif(output.startswith('＜ｅｘｔｒａ＿ｉｄ＿９７＞')):
		return 'exo2'
	
	dep_trees = dep_trees_dict[psa_instance['ntc_path']]
	ntc_sentences = ntc_sentences_dict[psa_instance['ntc_path']]
	pred_bunsetsu_index = psa_instance['pred_bunsetsu_index']
	pred_sent_index = psa_instance['pred_sent_index']

	s = 0
	b = 0

	candidates = []
	for sentence in ntc_sentences:
		for bunsetsu in sentence:
			if output in bunsetsu:
				candidates.append((s,b))
			b += 1
		s += 1
		b = 0 
	nearest_arg_type = arg_index['none']
	for candidate in candidates:
		if(pred_sent_index != candidate[0]):
			arg_type = arg_index['inter']
		elif(pred_bunsetsu_index == dep_trees[candidate[0]][candidate[1]] or candidate[1] == dep_trees[pred_sent_index][pred_bunsetsu_index]):
			arg_type = arg_index['dep']
		else:
			arg_type = arg_index['intra']
		if nearest_arg_type > arg_type:
			nearest_arg_type = arg_type
	return arg_inv_index[nearest_arg_type]
				

def parse_ntc_bunsetsu(ntc_text:str)->list:

	sentences = []
	for line in ntc_text.splitlines():
		if line.startswith('#'):
			sentence = []
			bunsetsu = ''
		elif line.startswith('*'):
			if len(bunsetsu) > 0 :
				sentence.append(bunsetsu)
			bunsetsu = ''
		elif line.startswith('EOS'):
			sentence.append(bunsetsu)
			sentences.append(sentence)
		else:
			bunsetsu += line.split()[0]
	return sentences


def make_dfdict(decoded_path:str)->dict:
	results = {
		'dep':{'tp':[],'fp':[],'fn':[]},
		'intra':{'tp':[],'fp':[],'fn':[]},
		'inter':{'tp':[],'fp':[],'fn':[]},
		'exog':{'tp':[],'fp':[],'fn':[]},
		'exo1':{'tp':[],'fp':[],'fn':[]},
		'exo2':{'tp':[],'fp':[],'fn':[]}
	}

	f = open(decoded_path,mode='r')
	lines = f.readlines()
	for line in lines:
		psa_instance= json.loads(line)
		gold_arg_type = psa_instance['arg_type']
		sys_arg_type = search_argtype(psa_instance)
		output = psa_instance['output'] if sys_arg_type != 'none' else ''
		# if(sys_arg_type == 'none'):
		# 	print(f'output {psa_instance["output"]}\n context{psa_instance["context"]}')
		if(gold_arg_type == 'none'):
			if(output == ''):
				continue
			else:
				results[sys_arg_type]['fp'].append(psa_instance)
		elif output != '':
			answer = is_correct(psa_instance['output'],psa_instance['gold_arguments'],subword=subword,no_distinct=no_distinct,bidirection=bidirection)
			if(answer):
				results[gold_arg_type]['tp'].append(psa_instance)
			else:
				results[sys_arg_type]['fp'].append(psa_instance)
				results[gold_arg_type]['fn'].append(psa_instance)
		else:
			results[gold_arg_type]['fn'].append(psa_instance)
	return results

parser = create_parser()
args = parser.parse_args()
subword = args.subword
bidirection = args.bidirection
no_distinct = args.no_distinct


ntc_paths = glob.glob(os.path.join(args.ntc_dir,'*'))
dep_trees_dict = {} #ntcのパスをkeyとし、その文章のDEP treeをvalueとする
ntc_sentences_dict = {} #ntcのパスをkeyとし、文章のリストをvalueとする
for ntc_path in ntc_paths:
	with open(ntc_path,mode='r') as f:
		ntc_text = f.read()
	dep_trees = make_dep_trees(ntc_text=ntc_text)
	ntc_sentences = parse_ntc_bunsetsu(ntc_text)
	dep_trees_dict[os.path.basename(ntc_path)] = dep_trees
	ntc_sentences_dict[os.path.basename(ntc_path)] = ntc_sentences

bert_tokenizer = AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')

results = make_dfdict(args.decode)

for k,result in results.items():
	print_scores(k,result)