import json
import argparse
import glob
import os
import codecs
from pprint import pprint
from tqdm import tqdm
from transformers import AutoTokenizer
from NTC2PSAdataset import make_dep_trees,determin_argtype

arg_index = {'dep':0,'intra':1,'inter':2,'exo':3,'none':4}
arg_inv_index = {0:'dep',1:'intra',2:'inter',3:'exo',4:'none'}

arg_types = ['dep','intra','inter','exog','exo1','exo2','exdoc']

def create_parser():
	parser = argparse.ArgumentParser()
	parser.add_argument('--decode','-d',help='path to decoded jsonl generated by generate.py')
	parser.add_argument('--ntc_dir','-n',help='path to NTC testset')
	parser.add_argument('--output_dir','-o',help='path to output_dir')
	parser.add_argument('--subword','-s',action='store_true',help='Matching subword endings are correct')
	parser.add_argument('--no_distinct',action='store_true',help="Don't distinct subword token and normal token")
	parser.add_argument('--bidirection','-b',action='store_true',help='出力がgoldを含むかgoldに出力が含まれれば正解とする')

	return parser

def print_scores(results):
	with open(os.path.join(output_dir,'score.log'),mode='w') as f:
		for arg_type,result in results.items():
			tp = len(result['tp'])
			fp = len(result['fp'])
			fn = len(result['fn'])
			
			precision = tp/(tp + fp) if tp + fp > 0 else 0
			recall = tp/(tp + fn) if tp + fp > 0 else 0
			f1 = (2*recall*precision/(recall + precision)) if recall + precision != 0 else 0

			sys_arg_count = {arg_type:0 for arg_type in arg_types+['none']} 
			for psa_instance in result['tp'] + result['fn']:
				sys_arg_count[psa_instance['sys_arg_type']] += 1
			
			print(f'arg_type = {arg_type}\nprecision : {precision} ({tp}/{tp + fp})\nrecall : {recall} ({tp}/{tp + fn})\nf1 : {f1}',file=f)
			print(f'sys output type {sys_arg_count}\n',file=f)

def decode_result(result:dict):

	for arg_type,result in results.items():
		if not os.path.exists(os.path.join(output_dir,arg_type)):
			os.makedirs(os.path.join(output_dir,arg_type))
		for tf,psas in result.items():
			with open (os.path.join(output_dir,arg_type,tf),mode='w',encoding='utf-8') as f:
				for psa in psas:
					print(psa,file=f)

def is_correct(output:str,gold_arguments:list,subword=False,no_distinct=False,bidirection=False)->bool:
	"""
	* subword : 評価の際にトークンをサブワード化した末尾が一致していれば正解とする
	* no_distinct : 評価の際に[#糸]と[糸]のようなサブワードトークンと単一トークンを区別しない
	"""
	output_token = output.translate(str.maketrans({chr(0x0021 + i): chr(0xFF01 + i) for i in range(94)})) #半角文字を全角文字に変換　
	gold_arguments = [gold.translate(str.maketrans({chr(0x0021 + i): chr(0xFF01 + i) for i in range(94)})) for gold in gold_arguments]
	if(subword == True and gold_arguments not in [['<extra_id_99>'],['<extra_id_98>'],['<extra_id_97>']]):
		output_token = bert_tokenizer.tokenize(output_token)[-1]
		gold_arguments = [bert_tokenizer.tokenize(argument)[-1] for argument in gold_arguments]
		if(no_distinct):
			output_token = output_token.replace('#','')
			gold_arguments = [argument.replace('#','') for argument in gold_arguments]

	result = False
	if(bidirection):
		for gold in gold_arguments:
			if ((output_token in gold) or (gold in output_token)):
				result = True
	else:
		if output_token in gold_arguments:
			result = True
	return result

def search_argtype(psa_instance)->str:
	output = psa_instance['output'].translate(str.maketrans({chr(0x0021 + i): chr(0xFF01 + i) for i in range(94)}))
	if(output == ''):
		return 'none'
	elif(output.startswith('＜ｅｘｔｒａ＿ｉｄ＿９９＞')):
		return 'exog'
	elif(output.startswith('＜ｅｘｔｒａ＿ｉｄ＿９８＞')):
		return 'exo1'
	elif(output.startswith('＜ｅｘｔｒａ＿ｉｄ＿９７＞')):
		return 'exo2'
	
	dep_trees = dep_trees_dict[psa_instance['ntc_path']]
	ntc_sentences = ntc_sentences_dict[psa_instance['ntc_path']]
	pred_bunsetsu_index = psa_instance['pred_bunsetsu_index']
	pred_sent_index = psa_instance['pred_sent_index']

	s = 0
	b = 0

	candidates = []
	for sentence in ntc_sentences:
		for bunsetsu in sentence:
			if output in bunsetsu:
				candidates.append((s,b))
			b += 1
		s += 1
		b = 0 

	if(len(candidates) == 0 ):
		return 'exdoc'
	
	nearest_arg_type = 100
	for candidate in candidates:
		if(pred_sent_index != candidate[0]):
			arg_type = arg_index['inter']
		elif(pred_bunsetsu_index == dep_trees[candidate[0]][candidate[1]] or candidate[1] == dep_trees[pred_sent_index][pred_bunsetsu_index]):
			arg_type = arg_index['dep']
		else:
			arg_type = arg_index['intra']
		if nearest_arg_type > arg_type:
			nearest_arg_type = arg_type
	return arg_inv_index[nearest_arg_type]
				

def parse_ntc_bunsetsu(ntc_text:str)->list:
	sentences = []
	for line in ntc_text.splitlines():
		if line.startswith('#'):
			sentence = []
			bunsetsu = ''
		elif line.startswith('*'):
			if len(bunsetsu) > 0 :
				sentence.append(bunsetsu)
			bunsetsu = ''
		elif line.startswith('EOS'):
			sentence.append(bunsetsu)
			sentences.append(sentence)
		else:
			bunsetsu += line.split()[0]
	return sentences


def make_dfdict(decoded_path:str)->dict:
	results = {arg_type:{'tp':[],'fp':[],'fn':[]} for arg_type in arg_types}

	f = open(decoded_path,mode='r')
	lines = f.readlines()
	for line in tqdm(lines):
		psa_instance= json.loads(line)
		gold_arg_type = psa_instance['arg_type']
		sys_arg_type = search_argtype(psa_instance)
		psa_instance['sys_arg_type'] = sys_arg_type
		output = psa_instance['output'] if sys_arg_type != 'exdoc' else ''
		# if(sys_arg_type == 'none'):
		# 	print(f'output {psa_instance["output"]}\n context{psa_instance["context"]}')
		if(gold_arg_type == 'none'):
			if(output == ''):
				continue
			else:
				results[sys_arg_type]['fp'].append(psa_instance)
		elif output != '':
			answer = is_correct(psa_instance['output'],psa_instance['gold_arguments'],subword=subword,no_distinct=no_distinct,bidirection=bidirection)
			if(answer):
				results[gold_arg_type]['tp'].append(psa_instance)
			else:
				results[sys_arg_type]['fp'].append(psa_instance)
				results[gold_arg_type]['fn'].append(psa_instance)
		else:
			results[gold_arg_type]['fn'].append(psa_instance)
	return results

parser = create_parser()
args = parser.parse_args()
subword = args.subword
bidirection = args.bidirection
no_distinct = args.no_distinct
output_dir = args.output_dir


ntc_paths = glob.glob(os.path.join(args.ntc_dir,'*'))
dep_trees_dict = {} #ntcのパスをkeyとし、その文章のDEP treeをvalueとする
ntc_sentences_dict = {} #ntcのパスをkeyとし、文章のリストをvalueとする
for ntc_path in ntc_paths:
	with open(ntc_path,mode='r') as f:
		ntc_text = f.read()
	dep_trees = make_dep_trees(ntc_text=ntc_text)
	ntc_sentences = parse_ntc_bunsetsu(ntc_text)
	dep_trees_dict[os.path.basename(ntc_path)] = dep_trees
	ntc_sentences_dict[os.path.basename(ntc_path)] = ntc_sentences

if(args.subword):
	bert_tokenizer = AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')

results = make_dfdict(args.decode)
if not os.path.exists(output_dir):
	os.makedirs(output_dir)

decode_result(results)
print_scores(results)