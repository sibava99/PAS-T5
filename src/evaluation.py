import json
import argparse
import glob
import os
from pprint import pprint
# from transformers import AutoTokenizer
from NTC2PSAdataset import make_dep_trees,determin_argtype

arg_index = {'dep':0,'intra':1,'inter':2,'exo':3,'none':4}
arg_inv_index = {0:'dep',1:'intra',2:'inter',3:'exo',4:'none'}

# decode_dict = {
# 					'output':argument,
# 					'gold_arguments':psa_instance['gold_arguments'][case],
# 					'case_name':case,
# 					'arg_type':psa_instance['arg_types'][case],
# 					'predicate':predicate,
# 					'pred_sent_index':psa_instance['pred_sent_index'],
# 					'pred_indices':psa_instance['pred_indices'],
# 					'alt_type':alt_type,
# 					'output_sentence':t5_tokenizer.decode(output),
# 					'context':psa_instance['context'],
# 					'ntc_path':psa_instance['ntc_path']
# 					}
def create_parser():
	parser = argparse.ArgumentParser()
	parser.add_argument('--decode','-d',help='path to decoded jsonl generated by generate.py')
	parser.add_argument('--ntc_dir','-n',help='path to NTC testset')

	return parser

def print_scores(df_name:str,df):
	value_count = df['result'].value_counts()
	tp = value_count['tp']
	fp = value_count['fp']
	fn = value_count['fn']
	
	precision = tp/(tp + fp)
	recall = tp/(tp + fn + fp)
	
	f1 = 2*recall*precision/(recall + precision)
	print(f'DataFrame = {df_name}\nprecision : {precision} ({tp}/{tp + fp})\nrecall : {recall} ({tp}/{len(df)})\nf1 : {f1}\n')

def is_correct(output:str,gold_arguments:list,subword=False,no_distinct=False,bidirection=False)->bool:
	"""
	* subword : 評価の際にトークンをサブワード化した末尾が一致していれば正解とする
	* no_distinct : 評価の際に[#糸]と[糸]のようなサブワードトークンと単一トークンを区別しない
	"""
	output_token = output.translate(str.maketrans({chr(0x0021 + i): chr(0xFF01 + i) for i in range(94)})) #半角文字を全角文字に変換
	gold_arguments = [gold.translate(str.maketrans({chr(0x0021 + i): chr(0xFF01 + i) for i in range(94)})) for gold in gold_arguments]
	if(subword == True):
		output_token = bert_tokenizer.tokenize(output_token)[-1]
		gold_arguments = [bert_tokenizer.tokenize(argument)[-1] for argument in gold_arguments]
		if(no_distinct):
			output_token = output_token.replace('#','')
			gold_arguments = [argument.replace('#','') for argument in gold_arguments]

	if(bidirection):
		for gold in gold_arguments:
			if (output_token in gold or gold in output_token):
				result = True
			else:
				result = False
	else:
		result = True if (output_token in gold_arguments) else False
	return result

def search_argtype(psa_instance)->str:
	output = psa_instance['output']
	if(output.startswith('<extra_id_99>')):
		return 'exog'
	elif(output.startswith('<extra_id_98>')):
		return 'exo1'
	elif(output.startswith('<extra_id_97>')):
		return 'exo2'
	
	dep_trees = dep_trees_dict[psa_instance['ntc_path']]
	ntc_sentences = ntc_sentences_dict[psa_instance['ntc_path']]
	pred_bunsetsu_index = psa_instance['pred_bunsetsu_index']
	pred_sent_index = psa_instance['pred_sent_index']

	s = 0
	b = 0

	candidates = []
	for sentence in ntc_sentences:
		for bunsetsu in sentence:
			if output in bunsetsu:
				candidates.append((s,b))
				print(bunsetsu)
			b += 1
		s += 1
		b = 0 
	assert len(candidates) > 0
	nearest_arg_type = arg_index['none']
	print(f'output {output} pred_sent_index {pred_sent_index},dep tree{dep_trees[pred_sent_index]}')
	for candidate in candidates:
		print(candidate)
		if(pred_sent_index != candidate[0]):
			arg_type = arg_index['inter']
		elif(pred_bunsetsu_index == dep_trees[candidate[0]][candidate[1]] or candidate[1] == dep_trees[pred_sent_index][pred_bunsetsu_index]):
			arg_type = arg_index['dep']
			print('dep')
		else:
			arg_type = arg_index['intra']
		if nearest_arg_type > arg_type:
			nearest_arg_type = arg_type
	return arg_inv_index[nearest_arg_type]
				

def parse_ntc_bunsetsu(ntc_text:str)->list:
	sentences = []
	for line in ntc_text.splitlines():
		if line.startswith('#'):
			sentence = []
			bunsetsu = ''
		elif line.startswith('*'):
			if len(bunsetsu) > 0 :
				sentence.append(bunsetsu)
			bunsetsu = ''
		elif line.startswith('EOS'):
			sentence.append(bunsetsu)
			sentences.append(sentence)
		else:
			bunsetsu += line.split()[0]
	return sentences


def make_dfdict(decoded_path:str)->dict:
	results = {
		'dep':{'tp':[],'fp':[],'fn':[]},
		'intra':{'tp':[],'fp':[],'fn':[]},
		'inter':{'tp':[],'fp':[],'fn':[]},
		'exog':{'tp':[],'fp':[],'fn':[]},
		'exo1':{'tp':[],'fp':[],'fn':[]},
		'exo2':{'tp':[],'fp':[],'fn':[]}
	}

	f = open(decoded_path,mode='r')
	lines = f.readlines()
	for line in lines:
		psa_instance= json.loads(line)
		gold_arg_type = psa_instance['arg_type']
		if(gold_arg_type == 'none'):
			if(psa_instance['output'] == ''):
				continue
			else:
				sys_arg_type = search_argtype(psa_instance)
				results[sys_arg_type]['fp'].append(psa_instance)
		elif psa_instance['output'] != '':
			answer = is_correct(psa_instance['output'],psa_instance['gold_arguments'])
			if(answer):
				results[gold_arg_type]['tp'].append(psa_instance)
			else:
				sys_arg_type = search_argtype(psa_instance)
				results[sys_arg_type]['fp'].append(psa_instance)
				results[gold_arg_type]['fn'].append(psa_instance)
		else:
			results[gold_arg_type]['fn'].append(psa_instance)
	return results

parser = create_parser()
args = parser.parse_args()

ntc_paths = glob.glob(os.path.join(args.ntc_dir,'*'))
dep_trees_dict = {} #ntcのパスをkeyとし、その文章のDEP treeをvalueとする
ntc_sentences_dict = {} #ntcのパスをkeyとし、文章のリストをvalueとする
for ntc_path in ntc_paths:
	with open(ntc_path,mode='r') as f:
		ntc_text = f.read()
	dep_trees = make_dep_trees(ntc_text=ntc_text)
	ntc_sentences = parse_ntc_bunsetsu(ntc_text)
	dep_trees_dict[os.path.basename(ntc_path)] = dep_trees
	ntc_sentences_dict[os.path.basename(ntc_path)] = ntc_sentences

# bert_tokenizer = AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')
# t5_tokenizer = AutoTokenizer.from_pretrained('megagonlabs/t5-base-japanese-web')

results = make_dfdict(args.decode)

pprint(results)


