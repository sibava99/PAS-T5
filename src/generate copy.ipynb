{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sibava/miniconda3/envs/psat5/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import pprint\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    T5Config\n",
    ")\n",
    "import MeCab\n",
    "import unidic\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数シードの設定\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_t5tok_2_sptok(t5_text:str) -> str:\n",
    "\tt5_text = t5_text.replace('<pad>','')\n",
    "\tt5_text = t5_text.replace('</s>','')\n",
    "\tt5_text = t5_text.replace('<extra_id_99>','→')\n",
    "\tt5_text = t5_text.replace('<extra_id_98>','↑')\n",
    "\tt5_text = t5_text.replace('<extra_id_97>','←')\n",
    "\treturn t5_text\n",
    "def replace_sptok_2_t5tok(sp_text:str) -> str:\n",
    "\tsp_text = sp_text.replace('→','<extra_id_99>')\n",
    "\tsp_text = sp_text.replace('↑','<extra_id_98>')\n",
    "\tsp_text = sp_text.replace('←','<extra_id_97>')\n",
    "\treturn sp_text\n",
    "def get_index(l, x, default=False):\n",
    "    return l.index(x) if x in l else -256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrace_argument(output_ids:list,case:str,is_alt:bool=False) -> str:\n",
    "    sep_id = mega_tokenizer.encode('<extra_id_2>',add_special_tokens=None)[0]\n",
    "    if(is_alt):\n",
    "        if(sep_id in output_ids.tolist()):\n",
    "            sep_index = output_ids.tolist().index(sep_id)\n",
    "            output_ids = output_ids[sep_index + 1:]\n",
    "        else:\n",
    "            print('sep token does not exist in output tokens')\n",
    "    ja_case = case_en_ja[case]\n",
    "    output_tokens = tagger.parse(replace_t5tok_2_sptok(mega_tokenizer.decode(output_ids))).split()\n",
    "    # decoded_str = replace_t5tok_2_sptok(mega_tokenizer.decode(output_ids))\n",
    "    # decoded_str = re.sub(r'\\s','',decoded_str)\n",
    "    # juman_result = jumanpp.analysis(decoded_str)\n",
    "    # output_tokens = [mrph.midasi for mrph in juman_result.mrph_list()]\n",
    "    #「頼もしさ」の「さ」がガ格の項とき出力は「さが政府に求められる」となり解析不可？\n",
    "    indices = [i for i, x in enumerate(output_tokens) if x == ja_case]\n",
    "    if(len(indices) < 1):\n",
    "        return ''\n",
    "    else:\n",
    "        return replace_sptok_2_t5tok(output_tokens[indices[0] - 1])\n",
    "        #サブワードの末尾を取る場合はこのトークンをsentencepieceでトークナイズしてから末尾を取る\n",
    "        #ガヲニを基準としてそこから前の部分を全部ラベルとする ex)党代表が雄弁に演説した　ガ格:党代表　or 代表,長めで学習してから評価で末尾のsubwordをとればよい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_argument(output_ids:list,case:str,is_alt:bool=False) -> str:\n",
    "    sep_id = mega_tokenizer.encode('<extra_id_2>',add_special_tokens=None)[0]\n",
    "    if(is_alt):\n",
    "        if(sep_id in output_ids.tolist()):\n",
    "            sep_index = output_ids.tolist().index(sep_id)\n",
    "            output_ids = output_ids[sep_index + 1:]\n",
    "        else:\n",
    "            print('sep token does not exist in output tokens')\n",
    "    decoded_str = mega_tokenizer.decode(output_ids)\n",
    "    decoded_str = re.sub(r'\\s|<pad>','',decoded_str) \n",
    "    ga_index = get_index(decoded_str,'が')\n",
    "    o_index = get_index(decoded_str,'を')\n",
    "    ni_index = get_index(decoded_str,'に')\n",
    "    \n",
    "    ja_case = case_en_ja[case]\n",
    "    if(case == 'ga'):\n",
    "        if(ga_index == -256):\n",
    "            return ''\n",
    "        else:\n",
    "            return decoded_str[:ga_index]\n",
    "    elif(case == 'o'):\n",
    "        if(o_index == -256):\n",
    "            return ''\n",
    "        elif(ga_index == -256):\n",
    "            return decoded_str[:o_index]\n",
    "        else:\n",
    "            return decoded_str[ga_index+1:o_index]\n",
    "    elif(case == 'ni'):\n",
    "        if(ni_index == -256):\n",
    "            return ''\n",
    "        elif(o_index != -256):\n",
    "            return decoded_str[o_index+1:ni_index]\n",
    "        elif(o_index == -256 and ga_index != 0):\n",
    "            return decoded_str[ga_index + 1:ni_index]\n",
    "        else:\n",
    "            return decoded_str[0:ni_index]\n",
    "    else:\n",
    "        return replace_sptok_2_t5tok(output_tokens[indices[0] - 1])\n",
    "        #サブワードの末尾を取る場合はこのトークンをsentencepieceでトークナイズしてから末尾を取る\n",
    "        #ガヲニを基準としてそこから前の部分を全部ラベルとする ex)党代表が雄弁に演説した　ガ格:党代表　or 代表,長めで学習してから評価で末尾のsubwordをとればよい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_tokenizer = T5Tokenizer.from_pretrained(\"megagonlabs/t5-base-japanese-web\")\n",
    "mega_model = T5ForConditionalGeneration.from_pretrained(\"megagonlabs/t5-base-japanese-web\").to(device)\n",
    "mega_model.resize_token_embeddings(len(mega_tokenizer))\n",
    "mega_model.load_state_dict(torch.load('/home/sibava/PAS-T5/models/closest_50/checkpoint-6500/pytorch_model.bin'))\n",
    "\n",
    "\n",
    "mega_model.eval()\n",
    "\n",
    "\n",
    "case_en_ja = {\n",
    "\t'ga':'が',\n",
    "\t'o':'を',\n",
    "\t'ni':'に',\n",
    "\t'yotte':'によって'\n",
    "}\n",
    "\n",
    "# tagger = MeCab.Tagger('-Owakati')\n",
    "# from pyknp import Jumanpp\n",
    "# jumanpp = Jumanpp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-157737582470aa31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/sibava/.cache/huggingface/datasets/json/default-157737582470aa31/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 2577.94it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 369.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/sibava/.cache/huggingface/datasets/json/default-157737582470aa31/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('json',data_files={\"test\":'/home/sibava/PAS-T5/datasets/suffix/test.doc.jsonl'})\n",
    "input_dataset = dataset.with_format(type='torch',columns=['input_ids'])\n",
    "\n",
    "tensor_dataloader = DataLoader(input_dataset['test'],batch_size=16)\n",
    "pas_dataset = iter(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1649/1649 [11:22<00:00,  2.42it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/home/sibava/PAS-T5/decoded/suffix.jsonl',mode='w') as f:\n",
    "\tpas_dataset = iter(dataset['test'])\n",
    "\tdataloader = DataLoader(dataset['test'],batch_size=16)\n",
    "\tfor batch_input in tqdm(iter(tensor_dataloader)):\n",
    "\t\toutputs = mega_model.generate(batch_input['input_ids'].to(device))\n",
    "\t\tfor output in outputs:\n",
    "\t\t\tpsa_instance = next(pas_dataset)\n",
    "\t\t\talt_type = psa_instance['alt_type']\n",
    "\t\t\tpredicate = psa_instance['predicate']\n",
    "\t\t\tfor case in ['ga','o','ni']:\n",
    "\t\t\t\tif (psa_instance['arg_types'][case] == None):\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\targument = extract_argument(output,case)\n",
    "\t\t\t\t# argument = mega_tokenizer.decode(output,skip_special_tokens=True)\n",
    "\t\t\t\tdecode_dict = {\n",
    "\t\t\t\t\t'output_token':argument,\n",
    "\t\t\t\t\t'gold_arguments':psa_instance['gold_arguments'][case],\n",
    "\t\t\t\t\t'case_name':case,\n",
    "\t\t\t\t\t'arg_type':psa_instance['arg_types'][case],\n",
    "\t\t\t\t\t'predicate':predicate,\n",
    "\t\t\t\t\t'alt_type':alt_type,\n",
    "\t\t\t\t\t'output_sentence':mega_tokenizer.decode(output),\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\tf.write(json.dumps(decode_dict) + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "85644eeca380b35874bb5238156bf82a7fba876716ebaf69f91f463ecb287447"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('psat5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
