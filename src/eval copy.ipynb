{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sibava/miniconda3/envs/psat5/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mセル2 を /home/sibava/PAS-T5/src/eval copy.ipynb\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfvcrc07/home/sibava/PAS-T5/src/eval%20copy.ipynb#ch0000001vscode-remote?line=0'>1</a>\u001b[0m bert_tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mcl-tohoku/bert-base-japanese\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bfvcrc07/home/sibava/PAS-T5/src/eval%20copy.ipynb#ch0000001vscode-remote?line=1'>2</a>\u001b[0m t5_tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mmegagonlabs/t5-base-japanese-web\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/psat5/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:528\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    525\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    526\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTokenizer class \u001b[39m\u001b[39m{\u001b[39;00mtokenizer_class_candidate\u001b[39m}\u001b[39;00m\u001b[39m does not exist or is not currently imported.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         )\n\u001b[0;32m--> 528\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    530\u001b[0m \u001b[39m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[39m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m~/miniconda3/envs/psat5/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1780\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1778\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1780\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   1781\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   1782\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   1783\u001b[0m     init_configuration,\n\u001b[1;32m   1784\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   1785\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1786\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1787\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1788\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/psat5/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1915\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1913\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   1914\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1915\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   1916\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   1917\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   1918\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1919\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1920\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/psat5/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:130\u001b[0m, in \u001b[0;36mT5TokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[39mif\u001b[39;00m extra_tokens \u001b[39m!=\u001b[39m extra_ids:\n\u001b[1;32m    125\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    126\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBoth extra_ids (\u001b[39m\u001b[39m{\u001b[39;00mextra_ids\u001b[39m}\u001b[39;00m\u001b[39m) and additional_special_tokens (\u001b[39m\u001b[39m{\u001b[39;00madditional_special_tokens\u001b[39m}\u001b[39;00m\u001b[39m) are provided to T5Tokenizer. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mIn this case the additional_special_tokens must include the extra_ids tokens\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m         )\n\u001b[0;32m--> 130\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    131\u001b[0m     vocab_file,\n\u001b[1;32m    132\u001b[0m     tokenizer_file\u001b[39m=\u001b[39;49mtokenizer_file,\n\u001b[1;32m    133\u001b[0m     eos_token\u001b[39m=\u001b[39;49meos_token,\n\u001b[1;32m    134\u001b[0m     unk_token\u001b[39m=\u001b[39;49munk_token,\n\u001b[1;32m    135\u001b[0m     pad_token\u001b[39m=\u001b[39;49mpad_token,\n\u001b[1;32m    136\u001b[0m     extra_ids\u001b[39m=\u001b[39;49mextra_ids,\n\u001b[1;32m    137\u001b[0m     additional_special_tokens\u001b[39m=\u001b[39;49madditional_special_tokens,\n\u001b[1;32m    138\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    139\u001b[0m )\n\u001b[1;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39m=\u001b[39m vocab_file\n\u001b[1;32m    142\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcan_save_slow_tokenizer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/psat5/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:112\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m TokenizerFast\u001b[39m.\u001b[39mfrom_file(fast_tokenizer_file)\n\u001b[1;32m    110\u001b[0m \u001b[39melif\u001b[39;00m slow_tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[39m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    113\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslow_tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     \u001b[39m# We need to create and convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     slow_tokenizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslow_tokenizer_class(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/psat5/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:1033\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer)\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1027\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAn instance of tokenizer class \u001b[39m\u001b[39m{\u001b[39;00mtokenizer_class_name\u001b[39m}\u001b[39;00m\u001b[39m cannot be converted in a Fast tokenizer instance. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1028\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo converter was found. Currently available slow->fast convertors: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1029\u001b[0m     )\n\u001b[1;32m   1031\u001b[0m converter_class \u001b[39m=\u001b[39m SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n\u001b[0;32m-> 1033\u001b[0m \u001b[39mreturn\u001b[39;00m converter_class(transformer_tokenizer)\u001b[39m.\u001b[39mconverted()\n",
      "File \u001b[0;32m~/miniconda3/envs/psat5/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:425\u001b[0m, in \u001b[0;36mSpmConverter.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    421\u001b[0m requires_backends(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprotobuf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    423\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs)\n\u001b[0;32m--> 425\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m sentencepiece_model_pb2 \u001b[39mas\u001b[39;00m model_pb2\n\u001b[1;32m    427\u001b[0m m \u001b[39m=\u001b[39m model_pb2\u001b[39m.\u001b[39mModelProto()\n\u001b[1;32m    428\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal_tokenizer\u001b[39m.\u001b[39mvocab_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/miniconda3/envs/psat5/lib/python3.9/site-packages/transformers/utils/sentencepiece_model_pb2.py:52\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m DESCRIPTOR \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mFileDescriptor(\n\u001b[1;32m     36\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentencepiece_model.proto\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m     package\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentencepiece\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     ),\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m _sym_db\u001b[39m.\u001b[39mRegisterFileDescriptor(DESCRIPTOR)\n\u001b[1;32m     46\u001b[0m _TRAINERSPEC_MODELTYPE \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mEnumDescriptor(\n\u001b[1;32m     47\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModelType\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m     full_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentencepiece.TrainerSpec.ModelType\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     49\u001b[0m     filename\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     50\u001b[0m     file\u001b[39m=\u001b[39mDESCRIPTOR,\n\u001b[1;32m     51\u001b[0m     values\u001b[39m=\u001b[39m[\n\u001b[0;32m---> 52\u001b[0m         _descriptor\u001b[39m.\u001b[39;49mEnumValueDescriptor(name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mUNIGRAM\u001b[39;49m\u001b[39m\"\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, number\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, options\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, \u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m     53\u001b[0m         _descriptor\u001b[39m.\u001b[39mEnumValueDescriptor(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBPE\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, number\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m),\n\u001b[1;32m     54\u001b[0m         _descriptor\u001b[39m.\u001b[39mEnumValueDescriptor(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWORD\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, number\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m),\n\u001b[1;32m     55\u001b[0m         _descriptor\u001b[39m.\u001b[39mEnumValueDescriptor(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCHAR\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, number\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m),\n\u001b[1;32m     56\u001b[0m     ],\n\u001b[1;32m     57\u001b[0m     containing_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     58\u001b[0m     options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     59\u001b[0m     serialized_start\u001b[39m=\u001b[39m\u001b[39m1121\u001b[39m,\n\u001b[1;32m     60\u001b[0m     serialized_end\u001b[39m=\u001b[39m\u001b[39m1174\u001b[39m,\n\u001b[1;32m     61\u001b[0m )\n\u001b[1;32m     62\u001b[0m _sym_db\u001b[39m.\u001b[39mRegisterEnumDescriptor(_TRAINERSPEC_MODELTYPE)\n\u001b[1;32m     64\u001b[0m _MODELPROTO_SENTENCEPIECE_TYPE \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mEnumDescriptor(\n\u001b[1;32m     65\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mType\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     66\u001b[0m     full_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentencepiece.ModelProto.SentencePiece.Type\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     serialized_end\u001b[39m=\u001b[39m\u001b[39m1943\u001b[39m,\n\u001b[1;32m     80\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/psat5/lib/python3.9/site-packages/google/protobuf/descriptor.py:755\u001b[0m, in \u001b[0;36mEnumValueDescriptor.__new__\u001b[0;34m(cls, name, index, number, type, options, serialized_options, create_key)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m, name, index, number,\n\u001b[1;32m    753\u001b[0m             \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,  \u001b[39m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[1;32m    754\u001b[0m             options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, create_key\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 755\u001b[0m   _message\u001b[39m.\u001b[39;49mMessage\u001b[39m.\u001b[39;49m_CheckCalledFromGeneratedFile()\n\u001b[1;32m    756\u001b[0m   \u001b[39m# There is no way we can build a complete EnumValueDescriptor with the\u001b[39;00m\n\u001b[1;32m    757\u001b[0m   \u001b[39m# given parameters (the name of the Enum is not known, for example).\u001b[39;00m\n\u001b[1;32m    758\u001b[0m   \u001b[39m# Fortunately generated files just pass it to the EnumDescriptor()\u001b[39;00m\n\u001b[1;32m    759\u001b[0m   \u001b[39m# constructor, which will ignore it, so returning None is good enough.\u001b[39;00m\n\u001b[1;32m    760\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained('megagonlabs/t5-base-japanese-web')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(df_name:str,df):\n",
    "\tvalue_count = df['result'].value_counts()\n",
    "\ttp = value_count['tp']\n",
    "\tfp = value_count['fp']\n",
    "\tfn = value_count['fn']\n",
    "\t\n",
    "\tprecision = tp/(tp + fp)\n",
    "\trecall = tp/(tp + fn + fp)\n",
    "\t\n",
    "\tf1 = 2*recall*precision/(recall + precision)\n",
    "\tprint(f'DataFrame = {df_name}\\nprecision : {precision} ({tp}/{tp + fp})\\nrecall : {recall} ({tp}/{len(df)})\\nf1 : {f1}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't5_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mセル4 を /home/sibava/PAS-T5/src/eval copy.ipynb\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bfvcrc07/home/sibava/PAS-T5/src/eval%20copy.ipynb#ch0000011vscode-remote?line=0'>1</a>\u001b[0m t5_tokenizer\u001b[39m.\u001b[39mdecode([\u001b[39m0\u001b[39m,\u001b[39m354\u001b[39m,\u001b[39m452\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 't5_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "t5_tokenizer.decode([0,354,452])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dfdict(decoded_path:str) ->dict:\n",
    "\tf = open(decoded_path,mode='r')\n",
    "\tdf_psa = pd.read_json(f,orient='records',lines=True)\n",
    "\tdf_psa['result'] = 'tn'\n",
    "\toutput_token = mega_tokenizer.decode(output,skip_special_tokens=True)\n",
    "\tfor index,row in df_psa.iterrows():\n",
    "\t\tif(row['output_token'] == ''):\n",
    "\t\t\tif(row['case_type'] == 'null'):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\telse:\n",
    "\t\t\t\tdf_psa.at[index,'result'] = 'fn'\n",
    "\t\telse:\n",
    "\t\t\toutput_token = row['output_token'].translate(str.maketrans({chr(0x0021 + i): chr(0xFF01 + i) for i in range(94)})) #半角文字を全角文字に変換\n",
    "\t\t\tgold_arguments = [gold.translate(str.maketrans({chr(0x0021 + i): chr(0xFF01 + i) for i in range(94)})) for gold in row['gold_arguments']]\n",
    "\t\t\t# subword_gold_argument = [bert_tokenizer.tokenize(argument)[-1] for argument in gold_arguments]\n",
    "\t\t\t# if(bert_tokenizer.tokenize(output_token)[-1] in subword_gold_argument):\n",
    "\t\t\t# if(output_token in gold_arguments):\n",
    "\t\t\tsubword_gold_argument = [bert_tokenizer.tokenize(argument)[-1].replace('#','') for argument in gold_arguments]\n",
    "\t\t\toutput_sub_char = bert_tokenizer.tokenize(output_token)[-1].replace('#','').translate(str.maketrans({chr(0x0021 + i): chr(0xFF01 + i) for i in range(94)}))\n",
    "\t\t\tin_flag = False\n",
    "\t\t\tfor arg_sub_char in gold_arguments:\n",
    "\t\t\t\tif (output_sub_char in arg_sub_char):\n",
    "\t\t\t\t\tin_flag = True\n",
    "\t\t\tif(in_flag):\n",
    "\t\t\t\tdf_psa.at[index,'result'] = 'tp'\n",
    "\t\t\telse:\n",
    "\t\t\t\tdf_psa.at[index,'result'] = 'fp'\n",
    "\t\n",
    "\tdf_dict = {\n",
    "\t\t'df_dep' : df_psa.query('case_type == \"dep\"'),\n",
    "\t\t'df_intra' : df_psa.query('case_type == \"intra\"'),\n",
    "\t\t'df_inter' : df_psa.query('case_type == \"inter\"'),\n",
    "\t\t'df_dep_passive' : df_psa.query('case_type == \"dep\" and alt_type == \"passive\"'),\n",
    "\t\t'df_intra_passive' : df_psa.query('case_type == \"intra\" and alt_type == \"passive\"'),\n",
    "\t\t'df_inter_passive' : df_psa.query('case_type == \"inter\" and alt_type == \"passive\"'),\n",
    "\t\t'df_exo' : df_psa.query('case_type == \"exog\" or case_type == \"exo1\" or case_type == \"exo2\"'),\n",
    "\t\t# 'df_zero': df_psa.query('case_type != \"null\" and case_type != \"exog\" and case_type != \"exo1\" and case_type != \"exo2\" and case_type != \"dep\"'),\n",
    "\t\t'df_zero': df_psa.query('case_type != \"null\" and case_type != \"dep\"'),\n",
    "\t}\n",
    "\t\n",
    "\treturn df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juman_dfdict = make_dfdict('/home/sibava/PAS-T5/decoded/decoded_psa_juman3.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab_dfdict = make_dfdict('/home/sibava/PAS-T5/decoded/decoded_psa_closest.jsonl')\n",
    "mecab_dfdict = make_dfdict('/home/sibava/PAS-T5/decoded/decoded_psa_closest_parse.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output_token</th>\n",
       "      <th>gold_arguments</th>\n",
       "      <th>case_name</th>\n",
       "      <th>case_type</th>\n",
       "      <th>predicate</th>\n",
       "      <th>alt_type</th>\n",
       "      <th>output_sentence</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>鳥</td>\n",
       "      <td>[ユリカモメ]</td>\n",
       "      <td>ga</td>\n",
       "      <td>inter</td>\n",
       "      <td>引っ掛けて</td>\n",
       "      <td>active</td>\n",
       "      <td>&lt;pad&gt; 鳥が糸を引っ掛けて&lt;/s&gt; &lt;pad&gt; &lt;pad&gt;</td>\n",
       "      <td>東京・上野の不忍池で、無残な姿の鳥が目立つ。片足が切れたユリカモメ釣り糸を&lt;extra_id...</td>\n",
       "      <td>fp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>鳥</td>\n",
       "      <td>[釣り糸]</td>\n",
       "      <td>ga</td>\n",
       "      <td>intra</td>\n",
       "      <td>取れ</td>\n",
       "      <td>active</td>\n",
       "      <td>&lt;pad&gt; 鳥が糸を取れ&lt;/s&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;</td>\n",
       "      <td>東京・上野の不忍池で、無残な姿の鳥が目立つ。片足が切れたユリカモメ釣り糸を引っ掛けて&lt;ext...</td>\n",
       "      <td>fp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>竹ぐし</td>\n",
       "      <td>[しが]</td>\n",
       "      <td>ga</td>\n",
       "      <td>intra</td>\n",
       "      <td>突き刺さった</td>\n",
       "      <td>active</td>\n",
       "      <td>&lt;pad&gt; 竹ぐしが首を突き刺さった&lt;/s&gt;</td>\n",
       "      <td>東京・上野の不忍池で、無残な姿の鳥が目立つ。片足が切れたユリカモメ釣り糸を引っ掛けて取れなく...</td>\n",
       "      <td>fp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>くし</td>\n",
       "      <td>[ゴム]</td>\n",
       "      <td>ga</td>\n",
       "      <td>intra</td>\n",
       "      <td>入って</td>\n",
       "      <td>active</td>\n",
       "      <td>&lt;pad&gt; くしが首に入って&lt;/s&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;</td>\n",
       "      <td>東京・上野の不忍池で、無残な姿の鳥が目立つ。片足が切れたユリカモメ釣り糸を引っ掛けて取れなく...</td>\n",
       "      <td>fp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>法人</td>\n",
       "      <td>[省]</td>\n",
       "      <td>ga</td>\n",
       "      <td>intra</td>\n",
       "      <td>多い</td>\n",
       "      <td>active</td>\n",
       "      <td>&lt;pad&gt; 法人が多い&lt;/s&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;</td>\n",
       "      <td>村山連立政権の最重要政策課題になっている特殊法人の整理・合理化で、通産省は日本貿易振興会とア...</td>\n",
       "      <td>fp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79038</th>\n",
       "      <td>さ</td>\n",
       "      <td>[官僚]</td>\n",
       "      <td>ga</td>\n",
       "      <td>intra</td>\n",
       "      <td>して</td>\n",
       "      <td>active</td>\n",
       "      <td>&lt;pad&gt; さが国民をして&lt;/s&gt; &lt;pad&gt; &lt;pad&gt;</td>\n",
       "      <td>法学部の一人の学生が書いた投稿が、元日付東京大学新聞に載る。投稿掲載は珍しくもないが、内容が...</td>\n",
       "      <td>fp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79044</th>\n",
       "      <td>社会</td>\n",
       "      <td>[日本, 国, 国民, 人]</td>\n",
       "      <td>ga</td>\n",
       "      <td>inter</td>\n",
       "      <td>経て</td>\n",
       "      <td>active</td>\n",
       "      <td>&lt;pad&gt; 社会が年を経て&lt;/s&gt; &lt;pad&gt; &lt;pad&gt;</td>\n",
       "      <td>法学部の一人の学生が書いた投稿が、元日付東京大学新聞に載る。投稿掲載は珍しくもないが、内容が...</td>\n",
       "      <td>fp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79083</th>\n",
       "      <td>コスト</td>\n",
       "      <td>[社会]</td>\n",
       "      <td>ga</td>\n",
       "      <td>intra</td>\n",
       "      <td>よい</td>\n",
       "      <td>active</td>\n",
       "      <td>&lt;pad&gt; コストがよい&lt;/s&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;</td>\n",
       "      <td>法学部の一人の学生が書いた投稿が、元日付東京大学新聞に載る。投稿掲載は珍しくもないが、内容が...</td>\n",
       "      <td>fp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79086</th>\n",
       "      <td>義務</td>\n",
       "      <td>[&lt;extra_id_99&gt;]</td>\n",
       "      <td>ga</td>\n",
       "      <td>exog</td>\n",
       "      <td>し</td>\n",
       "      <td>active</td>\n",
       "      <td>&lt;pad&gt; 義務が責任をし&lt;/s&gt; &lt;pad&gt;</td>\n",
       "      <td>法学部の一人の学生が書いた投稿が、元日付東京大学新聞に載る。投稿掲載は珍しくもないが、内容が...</td>\n",
       "      <td>fp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79089</th>\n",
       "      <td>&lt;extra_id_99&gt;</td>\n",
       "      <td>[者]</td>\n",
       "      <td>ga</td>\n",
       "      <td>intra</td>\n",
       "      <td>あう</td>\n",
       "      <td>active</td>\n",
       "      <td>&lt;pad&gt; &lt;extra_id_99&gt; があう&lt;/s&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;</td>\n",
       "      <td>法学部の一人の学生が書いた投稿が、元日付東京大学新聞に載る。投稿掲載は珍しくもないが、内容が...</td>\n",
       "      <td>fp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4807 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        output_token   gold_arguments case_name case_type predicate alt_type  \\\n",
       "9                  鳥          [ユリカモメ]        ga     inter     引っ掛けて   active   \n",
       "12                 鳥            [釣り糸]        ga     intra        取れ   active   \n",
       "21               竹ぐし             [しが]        ga     intra    突き刺さった   active   \n",
       "36                くし             [ゴム]        ga     intra       入って   active   \n",
       "105               法人              [省]        ga     intra        多い   active   \n",
       "...              ...              ...       ...       ...       ...      ...   \n",
       "79038              さ             [官僚]        ga     intra        して   active   \n",
       "79044             社会   [日本, 国, 国民, 人]        ga     inter        経て   active   \n",
       "79083            コスト             [社会]        ga     intra        よい   active   \n",
       "79086             義務  [<extra_id_99>]        ga      exog         し   active   \n",
       "79089  <extra_id_99>              [者]        ga     intra        あう   active   \n",
       "\n",
       "                                     output_sentence  \\\n",
       "9                    <pad> 鳥が糸を引っ掛けて</s> <pad> <pad>   \n",
       "12          <pad> 鳥が糸を取れ</s> <pad> <pad> <pad> <pad>   \n",
       "21                            <pad> 竹ぐしが首を突き刺さった</s>   \n",
       "36        <pad> くしが首に入って</s> <pad> <pad> <pad> <pad>   \n",
       "105    <pad> 法人が多い</s> <pad> <pad> <pad> <pad> <pad>   \n",
       "...                                              ...   \n",
       "79038                  <pad> さが国民をして</s> <pad> <pad>   \n",
       "79044                  <pad> 社会が年を経て</s> <pad> <pad>   \n",
       "79083             <pad> コストがよい</s> <pad> <pad> <pad>   \n",
       "79086                        <pad> 義務が責任をし</s> <pad>   \n",
       "79089  <pad> <extra_id_99> があう</s> <pad> <pad> <pad>   \n",
       "\n",
       "                                            input_tokens result  \n",
       "9      東京・上野の不忍池で、無残な姿の鳥が目立つ。片足が切れたユリカモメ釣り糸を<extra_id...     fp  \n",
       "12     東京・上野の不忍池で、無残な姿の鳥が目立つ。片足が切れたユリカモメ釣り糸を引っ掛けて<ext...     fp  \n",
       "21     東京・上野の不忍池で、無残な姿の鳥が目立つ。片足が切れたユリカモメ釣り糸を引っ掛けて取れなく...     fp  \n",
       "36     東京・上野の不忍池で、無残な姿の鳥が目立つ。片足が切れたユリカモメ釣り糸を引っ掛けて取れなく...     fp  \n",
       "105    村山連立政権の最重要政策課題になっている特殊法人の整理・合理化で、通産省は日本貿易振興会とア...     fp  \n",
       "...                                                  ...    ...  \n",
       "79038  法学部の一人の学生が書いた投稿が、元日付東京大学新聞に載る。投稿掲載は珍しくもないが、内容が...     fp  \n",
       "79044  法学部の一人の学生が書いた投稿が、元日付東京大学新聞に載る。投稿掲載は珍しくもないが、内容が...     fp  \n",
       "79083  法学部の一人の学生が書いた投稿が、元日付東京大学新聞に載る。投稿掲載は珍しくもないが、内容が...     fp  \n",
       "79086  法学部の一人の学生が書いた投稿が、元日付東京大学新聞に載る。投稿掲載は珍しくもないが、内容が...     fp  \n",
       "79089  法学部の一人の学生が書いた投稿が、元日付東京大学新聞に載る。投稿掲載は珍しくもないが、内容が...     fp  \n",
       "\n",
       "[4807 rows x 9 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab_dfdict['df_zero'].query('result == \"fp\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_juman_inter = juman_dfdict['df_inter']\n",
    "df_juman_inter = df_juman_inter.rename(columns={\"result\":\"juman_result\",\"output_token\":\"juman_output_token\"})\n",
    "df_juman_inter = df_juman_inter.loc[:,['juman_output_token','output_sentence','juman_result']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mecab_inter = mecab_dfdict['df_inter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df_mecab_inter.join(df_juman_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame = df_dep\n",
      "precision : 0.9464862923500539 (23718/25059)\n",
      "recall : 0.9154347909992666 (23718/25909)\n",
      "f1 : 0.9307016167006749\n",
      "\n",
      "DataFrame = df_intra\n",
      "precision : 0.7322658402203857 (4253/5808)\n",
      "recall : 0.6898621248986212 (4253/6165)\n",
      "f1 : 0.7104318048943455\n",
      "\n",
      "DataFrame = df_inter\n",
      "precision : 0.4992183428869203 (1916/3838)\n",
      "recall : 0.46811629611531885 (1916/4093)\n",
      "f1 : 0.48316731811877445\n",
      "\n",
      "DataFrame = df_dep_passive\n",
      "precision : 0.9224190592547342 (1510/1637)\n",
      "recall : 0.8688147295742232 (1510/1738)\n",
      "f1 : 0.8948148148148148\n",
      "\n",
      "DataFrame = df_intra_passive\n",
      "precision : 0.5914893617021276 (278/470)\n",
      "recall : 0.5440313111545988 (278/511)\n",
      "f1 : 0.5667686034658511\n",
      "\n",
      "DataFrame = df_inter_passive\n",
      "precision : 0.29222520107238603 (109/373)\n",
      "recall : 0.26520681265206814 (109/411)\n",
      "f1 : 0.2780612244897959\n",
      "\n",
      "DataFrame = df_exo\n",
      "precision : 0.7031208322219259 (2636/3749)\n",
      "recall : 0.6889702038682697 (2636/3826)\n",
      "f1 : 0.6959735973597361\n",
      "\n",
      "DataFrame = df_zero\n",
      "precision : 0.6573348264277715 (8805/13395)\n",
      "recall : 0.6251775063902301 (8805/14084)\n",
      "f1 : 0.640853015029659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for df_name,df in mecab_dfdict.items():\n",
    "\tprint_scores(df_name,df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "85644eeca380b35874bb5238156bf82a7fba876716ebaf69f91f463ecb287447"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('psat5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
